




```{r, echo = FALSE}
library(mice)
library(mlr3)
library(mlr3learners )
library(mlr3tuning)
library(mlr3mbo)
library(glmnet)
library(OpenML)
library(mlr3pipelines)
library(future)
library(magrittr)
library(data.table)
library(tidyverse)
library(corrr)
library(corrplot)
library(mlr3verse)
```



```{r, echo = FALSE}

data <- fread("Motor vehicle insurance data.csv", sep = ";")
```

# Looking at the data

```{r}
str(data)

num_data <- data %>% 
  dplyr::select(-Date_start_contract,
                -Date_last_renewal,
                -Date_next_renewal,
                -Date_birth,
                -Distribution_channel,
                -Date_lapse,
                -Date_driving_licence,
                -Type_fuel,
                -Length)
M <- stats::cor(num_data)
corrplot::corrplot(M, order = 'AOE')





```

We notice that there are multiple clusters, which indicate high correlation between some features. It could be desirable to group these, or remove some, if we decide to choose a model whose error relies heavily on the number of parameters. 

```{r, echo = FALSE}

data <- fread("Motor vehicle insurance data.csv", sep = ";")

data_trans <- function(data){
  
  today <- Sys.Date()
  
  date_cols <- c("Date_start_contract", 
                 "Date_last_renewal",
                 "Date_next_renewal", 
                 "Date_lapse")
  data[, (date_cols) := lapply(.SD, as.IDate, format = "%d/%m/%Y"),
       .SDcols = date_cols]
  data[, Period_end :=
         fifelse(
           !is.na(Date_lapse) &
             Date_lapse >= Date_last_renewal &            
             Date_lapse <= Date_next_renewal,            
           Date_lapse,
           pmin(Date_next_renewal, today, na.rm = TRUE) 
         )]
  
  
  data[, Exposure_days  := as.numeric(Period_end - Date_last_renewal)]
  data[, Exposure_days  := pmax(Exposure_days, 0)]      
  data[, Exposure_unit  := fcase(
    between(Exposure_days, 364, 367), 1,
    default = Exposure_days/365.25
  )]
  
  
  key_vars <- c("ID","Year_matriculation","Power",
                "Cylinder_capacity","N_doors","Type_fuel","Weight")
  
  agg <- data[, .(
    Exposure               = sum(Exposure_unit),
    Policies_in_force_max  = max(Policies_in_force),
    Max_policies_max       = max(Max_policies),
    Max_products_max       = max(Max_products),
    Payment_max            = max(Payment),
    # Premium_mean           = mean(Premium),
    Cost_claims_year_sum   = sum(Cost_claims_year),
    # N_claims_year_sum      = sum(N_claims_year),
    # N_claims_history_sum   = sum(N_claims_history),
    # R_claims_history_sum   = sum(R_Claims_history),
    Type_risk_max          = max(Type_risk),
    Value_vehicle_mean     = mean(Value_vehicle),
    N_doors_mean           = mean(N_doors),
    Length_sum             = sum(Length, na.rm = TRUE)
  ), by = key_vars]
  
  
  
  
  
  agg <- agg %>% 
    dplyr::mutate(claim_indicator = dplyr::if_else(Cost_claims_year_sum > 0, 1, 0))
  
  tmp <- agg %>% 
    dplyr::filter(is.na(Type_fuel))
  NA_fuel_Y <- sum(tmp$Cost_claims_year_sum)
  total_Y <- sum(agg$Cost_claims_year_sum)
  
  na_Y_ratio <- NA_fuel_Y / total_Y
  
  agg <- agg %>% 
    dplyr::filter(!is.na(Type_fuel))
  
  agg <- agg %>%
    dplyr::mutate(Type_fuel = dplyr::if_else(Type_fuel == "P", 1, 0))
  return(agg)
}


# data <- fread("Motor vehicle insurance data.csv", sep = ";")
# 
# X <- data[, -c("Cost_claims_year")]
# y <- data$Cost_claims_year
# 
# train_idx <- sample(seq_along(y), 10000)
# test_idx <- sample(setdiff(seq_along(y), train_idx), 10000)
# 
# train_X <- X[train_idx]
# train_y <- y[train_idx]
# test_X <- X[test_idx]
# test_y <- y[test_idx]

# vi skal gøre sådan her. 
train_combined <- train_X %>% dplyr::mutate(Cost_claims_year = train_y)

dat <- data_trans(train_combined)  
  

 
# mice::md.pattern(dat, plot = TRUE)



```




```{r}

ggplot2::ggplot(data = agg, ggplot2::aes(x = log(Cost_claims_year_sum))) +
  ggplot2::geom_density()




```

```{r}
# tweedie
library(mlr3learners)
library(lightgbm)
library(mlr3extralearners)

task_twd = as_task_regr(agg, target = "Cost_claims_year_sum", 
                        id = "tweedie")


graph_twd = po("encode") %>>%
  po("scale") %>>%
  po("learner", lrn("regr.lightgbm",
    objective              = "tweedie",
    tweedie_variance_power = to_tune(1.1, 1.9),
    learning_rate          = to_tune(1e-3, 0.2, logscale = TRUE),
    num_leaves             = to_tune(16L, 128L),

    num_iterations         = to_tune(200L, 1000L)

  ))

glrn_twd = GraphLearner$new(graph_twd, id = "tweedie_lgbm")
resampling = rsmp("cv", folds = 5)
tuner = tnr("random_search")
terminator = trm("evals", n_evals = 5)
measure = msr("regr.mae")


at = AutoTuner$new(
  learner    = glrn_twd,
  resampling = resampling,
  measure    = measure,
  terminator = terminator,
  tuner      = tuner
)

at$train(task_twd)

# simuler det der vil være i testen 
train_combined <- train_X %>% dplyr::mutate(Cost_claims_year = train_y)

dat <- data_trans(train_combined)  

task_x  <- as_task_regr(
  x = dat,
  target = "Cost_claims_year_sum",
  id = "newdata"
)


pred_x = at$predict(task_twd)
dat <- agg

# 1) Performance‐mål
mse  <- mean((pred_x$response - dat$Cost_claims_year_sum)^2)
rmse <- sqrt(mse)
mae  <- mean(abs(pred_x$response - dat$Cost_claims_year_sum))
r2   <- cor(pred_x$response, dat$Cost_claims_year_sum)^2
rmsle<- sqrt(mean((log1p(pred_x$response) - log1p(dat$Cost_claims_year_sum))^2))
             
cat("MSE: ",  mse,  "\n")
cat("RMSE:", rmse, "\n")
cat("MAE: ",  mae,  "\n")
cat("R²:  ",  r2,   "\n")
cat("rmsle:  ",  rmsle,   "\n")


ggplot(data = data.frame(
         obs  = log(dat$Cost_claims_year_sum+1),
         pred = log(pred_x$response+1)
       ),
       aes(x = pred, y = obs)
) +
  geom_point(alpha = 0.4) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(x = "Forudsagt", y = "Observeret",
       title = "Predicted vs Observed")

resid = dat$Cost_claims_year_sum - pred_x$response

# a) Histogram af residualer
ggplot(data.frame(resid = resid), aes(x = resid)) +
  geom_histogram(bins = 30, fill = "steelblue", alpha = 0.6) +
  labs(x = "Residual", title = "Histogram af residualer")

# b) QQ‐plot for at tjekke normalitet
ggplot(data.frame(resid = resid), aes(sample = resid)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "QQ‐plot af residualer")

# c) Residual vs forudsagt for at tjekke heteroskedasticitet
ggplot(data.frame(pred = pred_x$response, resid = resid),
       aes(x = pred, y = resid)
) +
  geom_point(alpha = 0.4) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(x = "Forudsagt", y = "Residual",
       title = "Residual vs Predicted")

```


We almost, in a pratical sense 
```{r}
data <- fread("Motor vehicle insurance data.csv", sep = ";")

data_F <- data_trans(data)

data_F <- data_F %>% dplyr::select(-Cost_claims_year_sum)


task_freq = as_task_classif(
  data_F,
  target = "claim_indicator",
  positive = "1",
  weights = data_F$Exposure,
  id     = "frek_binary"
)

task_freq$set_col_roles("Exposure", "weight")

graph_freq  = po("encode") %>>%
            po("scale")  %>>%
            lrn("classif.xgboost",
                 predict_type = "prob",
                 eval_metric  = "logloss",
                 nrounds      = to_tune(200, 800),
                 max_depth    = to_tune(3, 7),
                 eta          = to_tune(0.01, 0.3),
                 subsample    = to_tune(0.6, 1))

at_freq = auto_tuner(
  learner    = as_learner(graph_bin),
  resampling = rsmp("cv", folds = 5),
  measure    = msr("classif.bbrier"),
  tuner      = tnr("random_search"),
  term_evals = 5 
)


at_freq$train(task_freq)

pred_freq <- at_freq$predict(task_freq)


pred_freq_vec <- tibble::tibble(prob = pred_freq$prob[,1])
pred_freq_vec <- pred_freq_vec %>% dplyr::mutate(Y = dplyr::if_else(prob> 0.5, 1 , 0)) 


autoplot(pred_freq)
autoplot(pred_freq, type = "roc")
pred_freq$confusion


```


# model diagnostik

```{r}

data <- fread("Motor vehicle insurance data.csv", sep = ";")
data_S <- data_trans(data)

data_S <- data_F %>% dplyr::select(-claim_indicator)
data_S <- data_S %>% dplyr::filter(Cost_claims_year_sum > 0)



task_S = as_task_regr(
  data_S,
  target = "Cost_claims_year_sum",
  weights = data_S$Exposure,
  id     = "skade"
)

task_S$set_col_roles("Exposure", "weight")


graph_S = po("encode") %>>%
  po("scale") %>>%
  po("learner", lrn("regr.cv_glmnet",
    family           = "gaussian",
    alpha            = to_tune(0, 1),
    lambda.min.ratio = to_tune(1e-4, 1, logscale = TRUE)
  ))


glrn_S = GraphLearner$new(graph_S)

at_S = AutoTuner$new(
  learner    = glrn_S,
  resampling = rsmp("cv", folds = 5),
  measure    = msr("regr.mse"),
  tuner      = tnr("random_search"),
  terminator = trm("evals", n_evals = 5)
)

at_S$train(task_S)

pred_S <- at_S$predict(task_S)

pred_resp <- tibble::tibble(pred_S$response)



autoplot(pred_S)
autoplot(pred_S, type = "residual")
pred_freq$confusion


```










