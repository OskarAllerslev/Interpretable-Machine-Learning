




```{r, echo = FALSE}
library(mice)
library(mlr3)
library(mlr3learners )
library(mlr3tuning)
library(mlr3mbo)
library(glmnet)
library(OpenML)
library(mlr3pipelines)
library(future)
library(magrittr)
library(data.table)
library(tidyverse)
library(corrr)
library(corrplot)
library(mlr3verse)
```



```{r, echo = FALSE}

data <- fread("Motor vehicle insurance data.csv", sep = ";")
data

```

# Looking at the data

```{r}
str(data)

num_data <- data %>% 
  dplyr::select(-Date_start_contract,
                -Date_last_renewal,
                -Date_next_renewal,
                -Date_birth,
                -Distribution_channel,
                -Date_lapse,
                -Date_driving_licence,
                -Type_fuel,
                -Length)
M <- stats::cor(num_data)
corrplot::corrplot(M, order = 'AOE')





```

We notice that there are multiple clusters, which indicate high correlation between some features. It could be desirable to group these, or remove some, if we decide to choose a model whose error relies heavily on the number of parameters. 

```{r, echo = FALSE}

data <- fread("Motor vehicle insurance data.csv", sep = ";")

data_trans <- function(data){
  
  today <- Sys.Date()
  
  date_cols <- c("Date_start_contract", 
                 "Date_last_renewal",
                 "Date_next_renewal", 
                 "Date_lapse")
  data[, (date_cols) := lapply(.SD, as.IDate, format = "%d/%m/%Y"),
       .SDcols = date_cols]
  data[, Period_end :=
         fifelse(
           !is.na(Date_lapse) &
             Date_lapse >= Date_last_renewal &            
             Date_lapse <= Date_next_renewal,            
           Date_lapse,
           pmin(Date_next_renewal, today, na.rm = TRUE) 
         )]
  
  
  data[, Exposure_days  := as.numeric(Period_end - Date_last_renewal)]
  data[, Exposure_days  := pmax(Exposure_days, 0)]      
  data[, Exposure_unit  := fcase(
    between(Exposure_days, 364, 367), 1,
    default = Exposure_days/365.25
  )]
  

#data[, Cost_claims_sum := sum(Cost_claims_year), by = ID]

  # Add a column that flags the latest row per group
  
  setorder(data, ID, -Date_last_renewal)
  data[, is_second_latest := FALSE]
  data[, is_second_latest := .I == .I[2], by = ID]

  key_vars <- c("ID","Year_matriculation","Power",
                "Cylinder_capacity","N_doors","Type_fuel","Weight")
  
  agg <- data[, .(
    Exposure               = sum(Exposure_unit),
    Max_policies_max       = max(Max_policies),
    Policies_in_force_second_latest = Policies_in_force[is_second_latest],

    Max_products_max       = max(Max_products),
    Payment_max            = max(Payment),
    Premium_second_latest = Premium[is_second_latest],
    Cost_claim_this_year = Cost_claims_year[which.max(Date_last_renewal)],
    Cost_claims_sum_history = sum(Cost_claims_year) -(Cost_claims_year[which.max(Date_last_renewal)]),
    R_claims_history   = max(N_claims_history - N_claims_year)/sum(Exposure_unit),
    # N_claims_year_sum      = sum(N_claims_year),
    N_claims_history   = max(N_claims_history - N_claims_year),
    # R_claims_history_sum   = sum(R_Claims_history),
    Type_risk_max          = max(Type_risk),
    Value_vehicle_mean     = mean(Value_vehicle),
    N_doors_mean           = mean(N_doors),
    Length_sum             = sum(Length, na.rm = TRUE)
  ), by = key_vars]
  
  
  
  
  
  agg <- agg %>% 
    dplyr::mutate(claim_indicator = dplyr::if_else(Cost_claims_sum_history + Cost_claim_this_year > 0, 1, 0))
  
  tmp <- agg %>%
    dplyr::filter(is.na(Type_fuel))

  NA_fuel_Y <- sum(tmp$Cost_claims_sum_history + tmp$Cost_claims_this_year)
  total_Y <- sum(agg$Cost_claims_sum_history + agg$Cost_claims_this_year)

  na_Y_ratio <- NA_fuel_Y / total_Y
  
  agg <- agg %>% 
    dplyr::filter(!is.na(Type_fuel))
  
  agg <- agg %>%
    dplyr::mutate(Type_fuel = dplyr::if_else(Type_fuel == "P", 1, 0))
  return(agg)
}

 
# mice::md.pattern(dat, plot = TRUE)

agg = data_trans(data)


```



We almost, in a pratical sense 
```{r}

#Frekvensmodel

data <- fread("Motor vehicle insurance data.csv", sep = ";")

data_F <- data_trans(data)

data_F <- data_F %>% dplyr::select(-Cost_claim_this_year)


task_freq = as_task_classif(
  data_F,
  target = "claim_indicator",
  positive = "1",
  weights = data_F$Exposure,
  id     = "frek_binary"
)

task_freq$set_col_roles("Exposure", "weight")

graph_freq  = po("encode") %>>%
            po("scale")  %>>%
            lrn("classif.xgboost",
                 predict_type = "prob",
                 eval_metric  = "logloss", #Bedre loss funktion?
                 nrounds      = to_tune(200, 800),
                 max_depth    = to_tune(3, 7),
                 eta          = to_tune(0.01, 0.3),
                 subsample    = to_tune(0.6, 1))

at_freq = auto_tuner(
  learner    = as_learner(graph_freq),
  resampling = rsmp("cv", folds = 2), #ÆNDRET 5 -> 2
  measure    = msr("classif.bbrier"),
  tuner      = tnr("random_search"),
  term_evals = 2 #Ændret 5 til 1
)


at_freq$train(task_freq)

pred_freq <- at_freq$predict(task_freq)


pred_freq_vec <- tibble::tibble(prob = pred_freq$prob[,1])
pred_freq_vec <- pred_freq_vec %>% dplyr::mutate(Y = dplyr::if_else(prob> 0.5, 1 , 0)) 
pred_freq_vec <- pred_freq_vec %>%  dplyr::mutate(ID = data_F$ID)

autoplot(pred_freq)
autoplot(pred_freq, type = "roc")
pred_freq$confusion

```

```{r}
# Load the necessary library
library(ggplot2)

# Plot the distribution of the response variable
# ggplot(data_F, aes(x = log(Cost_claim_this_year))) +
#   geom_histogram(binwidth = 0.1, fill = "skyblue", color = "black", alpha = 0.7) +
#   labs(
#     title = "Distribution of Cost_claims_year_sum",
#     x = "Cost_claims_year_sum",
#     y = "Frequency"
#   ) +
#   theme_minimal()




```


# model diagnostik

```{r}

#Skademodel

data <- fread("Motor vehicle insurance data.csv", sep = ";")
data_S <- data_trans(data)



# num_data <- data %>%
#   dplyr::select(-Date_start_contract,
#                 -Date_last_renewal,
#                 -Date_next_renewal,
#                 -Date_birth,
#                 -Distribution_channel,
#                 -Date_lapse,
#                 -Date_driving_licence,
#                 -Type_fuel,
#                 -Length,
#                 -Period_end)

# data_S <- data_S %>%
#   arrange(desc(Cost_claims_sum_history)) %>%
#   slice(-1:-5)  # Remove the top 5 rows

# data_S <- data_S %>%
#   arrange(desc(Cost_claim_this_year)) %>%
#   slice(-1:-5) 


data_S <- data_S %>% dplyr::select(-claim_indicator)
data_S <- data_S %>% dplyr::filter(Cost_claim_this_year > 0)

#num_data <- num_data %>% dplyr::filter(Cost_claims_year > 0)

task_S = as_task_regr(
  data_S,
  target = "Cost_claim_this_year",
  weights = data_S$Exposure,
  id     = "skade"
)


task_S$set_col_roles("Exposure", "weight")

# 
# graph_S = po("encode") %>>%
#   po("scale") %>>%
#   po("learner", lrn("regr.cv_glmnet",
#     family           = "gaussian",
#     alpha            = to_tune(0, 1),
#     lambda.min.ratio = to_tune(1e-4, 1, logscale = TRUE)
#   ))
# 

graph_S = po("encode") %>>%
  #po("scale") %>>%
  po("learner", lrn("regr.ranger"))

glrn_S = GraphLearner$new(graph_S)

at_S = AutoTuner$new(
  learner    = glrn_S,
  resampling = rsmp("cv", folds = 2),  #ÆNDRET 5 -> 2
  measure    = msr("regr.mse"),
  tuner      = tnr("random_search"),
  terminator = trm("evals", n_evals = 2)  #ÆNDRET 5 -> 1
)

at_S$train(task_S)

pred_S <- at_S$predict(task_S)



pred_skade_vec <- tibble::tibble(pred_resp = pred_S$response)
pred_skade_vec <- pred_skade_vec %>%  dplyr::mutate(ID = data_S$ID)

autoplot(pred_S)
autoplot(pred_S, type = "residual")

```


```{r}
#Kombineret model

 
# Frekvens alt grupperet data, 52.645 rækker

# Skade, grupperet data, men claim_size > 0 = 5.902 rækker

#E[F] * E[Y]

data_agg <- data_trans(data)

freq_predict <- at_freq$predict_newdata(data_agg %>% dplyr::select(-claim_indicator))

skade_predict <- at_S$predict_newdata(data_agg %>% dplyr::select(-Cost_claim_this_year))

plot_df <- tibble::tibble(
  Y= data_agg$Cost_claim_this_year,
  Y_hat = skade_predict$response 
  
)

hist(as.numeric(plot_df$Y))




a <- freq_predict$prob[, 1]
b <- skade_predict$response

dt <- tibble::tibble(
  freq = a, 
  skade = b
)
dt <- dt %>% 
  dplyr::mutate(predicted = freq * skade)

mse <- mean((dt$predicted - data_agg$Cost_claim_this_year)^2)


merged_data <- dplyr::left_join(pred_freq_vec, pred_skade_vec, by = "ID")

merged_data <- merged_data %>%
  dplyr::mutate(
    prob = coalesce(prob, 0),         # Replace NA in 'prob' with 0
    pred_resp = coalesce(pred_resp, 0) # Replace NA in 'pred_resp' with 0
  )

#Er preds ordered efter ID?

mse_AB <- mean(((merged_data$prob* merged_data$pred_resp) - data_agg$Cost_claim_this_year)^2)
mse_AB



```








