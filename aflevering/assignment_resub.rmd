---
title: "Assignment 1"
author: "Jakob og Oskar"
date: "2025"
output:
  pdf_document:
    fig_caption: yes
---


```{r, echo = FALSE}

knitr::opts_chunk$set(
  echo   = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  dpi = 300,
  fig.width = 6, fig.height = 4
)
```



```{r, echo = FALSE, results=FALSE }
  data_trans <- function(data){

    today <- Sys.Date()

    date_cols <- c("Date_start_contract",
                   "Date_last_renewal",
                   "Date_next_renewal",
                   "Date_lapse")
    data[, (date_cols) := lapply(.SD, as.IDate, format = "%d/%m/%Y"),
         .SDcols = date_cols]
    data[, Period_end :=
           fifelse(
             !is.na(Date_lapse) &
               Date_lapse >= Date_last_renewal &
               Date_lapse <= Date_next_renewal,
             Date_lapse,
             pmin(Date_next_renewal, today, na.rm = TRUE)
           )]


    data[, Exposure_days  := as.numeric(Period_end - Date_last_renewal)]
    data[, Exposure_days  := pmax(Exposure_days, 0)]
    data[, Exposure_unit  := fcase(
      between(Exposure_days, 364, 367), 1,
      default = Exposure_days/365.25
    )]


    key_vars <- c("ID","Year_matriculation","Power",
                  "Cylinder_capacity","N_doors","Type_fuel","Weight", "Length",
                  "Area", "Value_vehicle", "Date_birth", "Date_driving_licence", "Date_start_contract")
    data$Date_birth <- as.integer(
      base::format(
        base::as.Date(data$Date_birth, format = "%d/%m/%Y"),
        "%Y"
      )
    )

    data$Date_driving_licence <- as.integer(
      base::format(
        base::as.Date(data$Date_driving_licence, format = "%d/%m/%Y"),
        "%Y"
      )
    )
    data$Date_start_contract <- as.integer(
      base::format(
        base::as.Date(data$Date_start_contract, format = "%d/%m/%Y"),
        "%Y"
      )
    )
    data <- data %>% dplyr::filter(Exposure_unit != 0)
    agg <- data[, .(
      Exposure               = sum(Exposure_unit),
      Max_policies_max       = max(Max_policies),
      Max_lapse              = max(Lapse),
      Max_Date_birth         = max(Date_birth),
      Max_Date_driving_licence = max(Date_driving_licence),
      Max_Date_start_contract = max(Date_start_contract),
      dist_channel_0 = as.integer(any(Distribution_channel == "0")),
      dist_channel_1 = as.integer(any(Distribution_channel == "1")),
      dist_channel_2 = as.integer(any(!(Distribution_channel %in% c("0", "1")))),
      Max_products_max       = max(Max_products),
      Payment_max            = max(Payment),
      Mean_premium            = (sum(Premium) - Premium[which.max(Date_last_renewal)]) / (sum(Exposure_unit)),
      Cost_claim_this_year = Cost_claims_year[which.max(Date_last_renewal)],
      Cost_claims_sum_history = sum(Cost_claims_year) -(Cost_claims_year[which.max(Date_last_renewal)]),
      R_claims_history   = max(N_claims_history - N_claims_year)/sum(Exposure_unit),
      N_claims_history   = max(N_claims_history - N_claims_year),
      Type_risk_max          = max(Type_risk),
      Value_vehicle_mean     = mean(Value_vehicle),
      N_doors_mean           = mean(N_doors),
      Length_sum             = sum(Length, na.rm = TRUE)
    ), by = key_vars]


    agg <- agg %>%
      dplyr::mutate(claim_indicator = dplyr::if_else(Cost_claims_sum_history + Cost_claim_this_year > 0, 1, 0))

    agg <- agg %>%
      dplyr::mutate(Type_fuel = dplyr::if_else(Type_fuel == "P", 1, 0))

    length_model <- lm(
      Length ~ Cylinder_capacity + Weight + Value_vehicle,
      data = na.omit(agg)
    )
    fuel_model <- lm(
      Type_fuel ~ Cylinder_capacity + Weight + Value_vehicle,
      data = na.omit(agg)
    )

    agg <- agg %>%
      dplyr::mutate(
        pred_length = stats::predict(length_model, newdata = .),
        pred_type_fuel = stats::predict(fuel_model, newdata = .)
      ) %>%
      dplyr::mutate(
        Length = dplyr::if_else(is.na(Length), pred_length, Length),
        Type_fuel = dplyr::if_else(is.na(Type_fuel), pred_type_fuel, Type_fuel)
      ) %>%
      dplyr::select(-pred_length, -pred_type_fuel)


    return(agg)
  }


```



```{r, echo = FALSE, results = FALSE, warnings = FALSE}
library(rsample)
library(DiagrammeR)
library(gt)
library(skimr)
library(patchwork)
library(mlr3viz)
library(GGally)
library(tweedie)
library(mlr3extralearners)
library(Hmisc)
library(mice)
library(mlr3)
library(mlr3learners )
library(mlr3tuning)
library(mlr3mbo)
library(glmnet)
library(OpenML)
library(mlr3pipelines)
library(future)
library(magrittr)
library(data.table)
library(tidyverse)
library(corrr)
library(corrplot)
library(mlr3verse)
```

# Assignment 1 in interpretable machine learning
We are tasked with building a predictive regression model, with the best possible prediction.
This submission will be split up into several parts:

- Initial data preprocessing and overview
- Introduction into the mathematics
- Modelling and justification
- Comparative discussion

## Initial data preprocessing and overview

The given data has the form:

```{r, echo = FALSE}

data <- fread("Motor vehicle insurance data.csv", sep = ";")
print(colnames(data))

```



We are asked to predict the *Cost_claims_year* given the rest of the covariate-vector.
Initially it is important to note that out data is a classical insurance dataset, where we are given rows corresponding to insurance periode for a given contract.
There are several issues with this, since some contracts might overlap into multiple contracts, which can be identified by the ID. 
There are numerous char. vectors in the data, which can be seen here:
```{r, echo = FALSE, out.width="50%"}
print(str(data))
summary(data)
```
One could, model some of the char. vectors like proposed in the lectures, by

\begin{align*}
 X_i &= E(Y \mid X_i ) \\
 &=\int_Y y\ \mu(x, dy)
\end{align*}

where $\mu$ is the appropriate probability kernel, and we let $y$ be our response.
However, we choose to take the chararacter vectors and round them to yearly values, which then has a ordinal ordering and can thus be used as features.
Further we take and one-hot encode *Distribution_channel* by creating three new features which are either $1$ or $0$. Same for the type of fuel.


Next there are some missing values.
```{r ,  X axis on right shows rows where x is missing, and on the left the amount of rows where it is missing.", echo = FALSE,results=FALSE }

data_with_na <- data %>%
  dplyr::select(dplyr::where(~base::anyNA(.x)))



```



```{r, out.width="50%" , fig.cap="Missing data plot, right axis shows numer of missing columns in that row, and the left axis show how many rows have this missingness pattern",echo = FALSE, results="hide", message=FALSE}

invisible(mice::md.pattern(data_with_na, plot = TRUE))
```

It becomes apparent that there are missing values in the length and type_fuel variable.
We can see that the missing is overlapping in $1764$ rows. However the length variable suffers way heavier from missing compared to type_fuel.
In order to impute values, we assume the missing it completely at random for both features.
We consider correlated features to do imputation. We see from the correlation plot (fig. \ref{fig:corplot} ) that type_fuel is mostly correlated with Cylinder_capacity, Value_vehicle and Weight. The same goes for the feature Length. Therefore, we fit a multivariate linear regression model with these 3 covariates to predict both type_fuel and Length (using cbind in the response formula). Finally, we predict the missing values using this trained model.


Our response variable *Cost_claims_year* suffers from a few extreme values. We decide to remove these values to later on achieve a better model fit. In fig. \ref{fig:outlier_plot}, *N_claims_year* is plotted against *Cost_claims_year*, where at least 5-10 extreme values of *Cost_claims_year* are spotted.

```{r outlier_plot, out.width="50%",  fig.cap = "Claim costs over number of claims ", echo = FALSE}
#Plotting
ggplot(data, aes(x = N_claims_year, y = Cost_claims_year)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  labs(
       x = "N_claims_year",
       y = "Cost_claims_year") +
  theme_minimal()

#Removing top 10 outliers

data_clean <- data %>%
  arrange(desc(Cost_claims_year)) %>%
  slice(-(1:10))

data <- data_clean %>%
  slice_sample(prop = 1)

```



Finally we look at the correlation between the covariates.
```{r corplot, out.width="70%",  fig.cap = "Correlation plot between the continious covariates", echo = FALSE}
num_data <- data %>%
  mutate(
    Type_fuel_num = case_when(
      Type_fuel == "P" ~ 1,
      Type_fuel == "D" ~ 0,
      TRUE             ~ NA_real_
    ),
    Length_num = as.numeric(Length)
  ) %>%

  select(
    -Date_start_contract, -Date_last_renewal, -Date_next_renewal,
    -Date_birth, -Date_lapse, -Date_driving_licence,
    -Distribution_channel,
    -Type_fuel,
    -Length
  ) %>%

  select(where(is.numeric))

M <- cor(num_data, use = "pairwise.complete.obs")
corrplot(M, order = "AOE")
```

We notice some clusters, most meaningfull between *Cylinder_capacity*, *Value_vehicle* and *Weight* which is expected. We deem these to have significant predictive ability, and thus we choose to not remove these.
The top left cluster, will be ignored for now, since we will later introduce a data-transformation which affects this cluster in a high degree.



##### Data Aggregation 



For our data aggregation we want the ability to assume independence, which we have if we aggregate the rows into one policy.
There are several problems to tackle to achieve data in the desired format.
We have to both choose an appropriate aggregation, and we have to consider events where the insured object change.
We start off by identifying some unique-identifiers.
If these columns change, we deem the contract to insure a new car, or at least insure something that is independent of what was previously insured.
Among these unique identifiers are *Length*, *Weight*, *Power* etc. If these change, it probably means that the insured object has changed.


Next, we look at how we have aggregate data. Firstly we sum the exposure for each contract, since we will use this to weight in the models later on.
Next we use the max function on the number of policies, lapse, date of birth, date of driving licence, start of contract, products, payments, ratio of claims history, type of risk,.
For the premium we use the mean of the premium without the forward premium, that is the premium for the time we want to predict for.
For the value of the vehicle, and number of doors we use mean.
For the length we use sum since this is just a scaling.
For the variable *cost_claims_this_year* we use the cost of the claims in $\mathcal{F}_t$ where we are to predict data on $\mathcal{F}_{t+1}$.



So we have $Z_i(\omega_i) := (X_i(\omega_i), Y_i(\omega_i)): (\Omega_i, \mathcal{F}_i) \rightarrow ( \mathcal{X} \times \mathcal{Y}, \mathcal{B}(\mathcal{X} \times \mathcal{Y}))$, where by aggregating data row-wise we obtain independence on every set in the Borel-set induced.
However this is an empirical assumption which can be violated by catastrophe events.


## Introduction into the mathematical framework

Note the classical derivation of how to decompose a total claim sum. 
\begin{align*}
E(S(t) \mid Z=z) &= E\left(\sum_{i=1}^{N(t)} X_i \mid Z = z \right)\\
&= E\left( \left( \sum_{i=1}^{N(t)} X_i \mid Z = z \cap \mathcal{F}(t) \right)\right) \\
&=E\left( N E\left( X_i \mid Z = z \right)  \mid Z=z\right) \\
&= E(N \mid Z = z) E(X_1 \mid Z = z)
\end{align*}
We would however like to modify this slightly, later on. 
Further the general definition of the Tweedie law is  
\begin{align*}
P_{\theta, \sigma^2}(Y \in A) = \int_A \exp\left\{ \frac{\theta z - \kappa_p(\theta)}{\sigma^2} \right\}\nu_y(dz)
\end{align*}
Where $A \subset \mathbb{R}$ and measurable. $\theta$ is the canonical parameter, and $\kappa_p(\theta)$ is the cumulative function dependent on $p$ the so called tweedie power parameter. $\sigma^2$ is the dispersion parameters, and $\nu_\lambda$ is some sigma-finite base measure depending on the Lebesgue measure. 

since it can accommodate a zero-inflated distribution very well, we feel the need to introduce this model here.
Lastly we introduce the so-called *bbrier* measure, for classification models.

\begin{align*}
\frac{1}{n} \sum_{i=1}^n w_i(1\{X_i = 1 \} - P(X_i = 1))^2
\end{align*}

Which is the mean weighted square euclidean distance between the probability and the true value.


## Modelling and justification

In this section, we train and evauluate different models. 
For all the proposed models, we have caried out nested cross-validition. Also, we stick to 5 folds for both inner and outer resampling, with 2 evaluations.
We do stratified resampling in both the inner and outer cross-validation-loops, to ensure both zero and non-zero claims over the training and test set.
I final note, that goes for all our trained models, is that we introduce offset on the exposure. Meaning: Give more importance to observations with more exposure since, they represent more information. 
```{r}
#Common trans dataset to skip rerunning trans

data_trans <- data_trans(data)

```


```{r, echo = FALSE, results = FALSE}


prep_graph = po("encode") %>>%
  po("scale")

g_featureless =
  prep_graph %>>%
  po("learner",
     lrn("regr.featureless")
  )

g_lgbm =
  prep_graph %>>%
  po("learner",
     lrn("regr.lightgbm",
         objective              = "tweedie",
         tweedie_variance_power = to_tune(1, 1.9),
         learning_rate          = to_tune(1e-3, 0.2, logscale = TRUE),
         num_leaves             = to_tune(16L, 32L),
         num_iterations         = to_tune(200L, 1000L))
  )

g_xgb =
  prep_graph %>>%
  po("learner",
     lrn("regr.xgboost",
         objective              = "reg:tweedie",
         tweedie_variance_power = to_tune(1, 1.9),
         eta                    = to_tune(1e-3, 0.2, logscale = TRUE),
         max_depth              = to_tune(3L, 9L),
         nrounds                = to_tune(200L, 1000L))
  )

g_enet =
  prep_graph %>>%
  po("learner",
     lrn("regr.glmnet",
         alpha = to_tune(0, 1),
         s     = to_tune(1e-4, 1, logscale = TRUE))
  )


auto = function(graph, id)
{
  at = AutoTuner$new(
    learner = GraphLearner$new(graph, id = id),
    resampling = rsmp("cv", folds = 5),
    measure = msr("regr.mse"),
    tuner = tnr("random_search"),
    terminator = trm("evals", n_evals = 1)
  )
  invisible(at)
}

at_lgbm = auto(g_lgbm, "lgbm")
at_xgb = auto(g_xgb, "xgb")
at_enet = auto(g_enet, "enet")
at_featureless = auto(g_featureless, "featureless")


data_twd <- data_trans
data_twd <- data_twd %>%
  dplyr::select(-ID)

task_twd = as_task_regr(data_twd,
                        target = "Cost_claim_this_year",
                        weights = "Exposure",
                        id = "tweedie")

    task_twd$set_col_roles("Exposure", "weight")


# Set claim_indicator as the stratum variable
task_twd$col_roles$stratum = "claim_indicator"

# Exclude claim_indicator from features (so it won't be used for training)
task_twd$col_roles$feature = setdiff(task_twd$col_roles$feature, "claim_indicator")


task = list(task_twd)
learners = list(at_lgbm, at_xgb, at_enet, at_featureless)
design = benchmark_grid(task = task, learners = learners,
                        resampling = rsmp("cv", folds = 5))

kør_igen <- "nej"

if (!file.exists("~/Interpretable-Machine-Learning/aflevering/benchmark_tweedie.rds" ) | kør_igen == "ja") {
  bmr_t = benchmark(design, store_models = TRUE)
  saveRDS(bmr_t, "~/Interpretable-Machine-Learning/aflevering/benchmark_tweedie.rds")
} else {
  bmr_t = readRDS("~/Interpretable-Machine-Learning/aflevering/benchmark_tweedie.rds")
}


```




##### Tweedie
Initily we want to fit a Tweedie model on the entire data, since the Tweedie model is known for handeling zero-inflated distributions well.
We plot the distribution of the claim size. 
```{r, fig.cap="Distribution of claim size pr. unique contract", echo=FALSE, warning=FALSE}
library(patchwork)
p1 <- data_trans %>% ggplot2::ggplot() +
  ggplot2::geom_density(ggplot2::aes(x = Cost_claim_this_year), size = 1, color = "darkblue") +
  ggplot2::theme_minimal() +
  ggplot2::ggtitle(label = "Response")
  
p2 <- data_trans %>% ggplot2::ggplot() +
  ggplot2::geom_density(ggplot2::aes(x = log(Cost_claim_this_year)), size = 1, color = "darkred") +
  ggplot2::theme_minimal() +
  ggplot2::ggtitle(label = "Log transformed response")

(p1 + p2)


```


We apply both the Xgboost, Ligth-gbm and glmnet. 
For each of these we supply search-space information in the "**Parameter search space tweedie**" Table. 





```{r parameter_space_tweedie, echo = FALSE}

gt::gt(tibble::tibble(
  Model = c("Xgboost", "lgbm", "glmnet"), 
  tweedie_power = c("1-1.9", "1-1.9", ""), 
  learning_rate_eta = c("1e-3 - 0.2", "1e-3 - 0.2", ""), 
  leaves = c("16 - 32", "3 - 9", ""), 
  iterations= c("200 - 1000", "200 - 1000", ""), 
  s = c("", "", "1e-4 - 1"), 
  alpha = c("", "", "0 - 1") 
  )
) %>%
  tab_header(
    title = md("**Parameter search space tweedie**")
  )


```

We define the workflow as shown in figure \ref{fig:combined_graph_plot}. 

```{r combined_graph_plot, fig.cap = "Input and model flow",  echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.width=12, fig.height=8, dpi=300}

# Individual learners
featureless_learner = po("learner", lrn("regr.featureless", id = "featureless"))
lgbm_learner = po("learner", lrn("regr.lightgbm", id = "lgbm"))
xgb_learner  = po("learner", lrn("regr.xgboost", id = "xgb"))
enet_learner = po("learner", lrn("regr.glmnet", id = "enet"))

combined_learners = gunion(list(featureless_learner, lgbm_learner, xgb_learner, enet_learner))

combined_graph = prep_graph %>>% combined_learners

combined_graph$plot()

```

The results of the nested cross-validation are shown in figure \ref{fig:boxplot_tweedie_kun}. 

```{r boxplot_tweedie_kun,  fig.cap = "MSE boxplot", echo = FALSE}

autoplot(bmr_t, measure = msr("regr.mse"), type = "boxplot")


```


And finally the chosen hyperparameters are presented in Table "**Best parameters tweedie**". 
```{r, echo = FALSE, include = FALSE}


invisible({
  
bm_agg     = bmr_t$aggregate(msr("regr.mse"))
best_row   = which.min(bm_agg$regr.mse)

rr         = bmr_t$resample_result(best_row)
fold_best  = which.min(rr$score(msr("regr.mse"))$regr.mse)


auto_best  = rr$learners[[fold_best]]
auto_best$tuning_result



best_params = auto_best$learner$param_set$values   
})

```

```{r best_parameters_tweedie, echo = FALSE}

gt::gt(enframe(best_params, name = "parameter", value = "value")) %>%
  tab_header(
    title = md("**Best parameters tweedie**")
  )

```

The tweedie power parameter makes the model a compound gamma. 









##### Split model
Next we can look at the split model proposed in the mathematical frame, where we aim to create two models, and then predict in the following manner:
\begin{align*}
P(N > 0 \mid Z) \cdot  E(X \mid Z)
\end{align*}

This is unlike the traditional and proposed $E(N)E(X)$, but since we are asked to predict the next year, we feel like we can provide a suitable approximation with the above prediction.
This entails fitting both a frequency model and a severity model. We use the same ML models for fitting as used to fit the tweedie model, adding random forest to the group of learners. 
Hence we begin with the frequency regression model.

```{r, echo = FALSE, results=FALSE}

data_F <- data_trans
data_F <- data_F %>%
  dplyr::select(-Cost_claim_this_year, - ID)



task_freq <- as_task_classif(
  data_F,
  target = "claim_indicator",
  positive = "1",
  weights = "Exposure",
  id = "frequency"
)


task_freq$set_col_roles("Exposure", "weight")
    
# stratificer tjek at det faktisk er hvad der sker her
task_freq$col_roles$stratum = "claim_indicator"


prep_graph = po("encode") %>>%
  po("scale")

g_featureless_f =
  prep_graph %>>%
  po("learner", lrn("classif.featureless", predict_type = "prob"))

g_rf_f = po("encode") %>>%
  po("learner", lrn("classif.ranger", predict_type = "prob",
                    num.trees = to_tune(100L, 500L),
                    mtry = to_tune(1L, as.integer(sqrt(ncol(data_F)-2)))))


g_lgbm_f =
  prep_graph %>>%
  po("learner",
     lrn("classif.lightgbm",
         objective              = "binary",
         learning_rate          = to_tune(1e-3, 0.2, logscale = TRUE),
         num_leaves             = to_tune(16L, 32L),
         num_iterations         = to_tune(200L, 1000L))
  )

g_xgb_f =
  prep_graph %>>%
  po("learner",
     lrn("classif.xgboost",
      predict_type = "prob",
      eval_metric = "logloss",
      nrounds      = to_tune(200, 800),
      max_depth    = to_tune(3, 7),
      eta          = to_tune(0.01, 0.3),
      subsample    = to_tune(0.6, 1)
  ))

g_ranger_f <- prep_graph %>>%
  po("learner",
     lrn("regr.ranger",
         mtry = 5,
         min.node.size = 1,  # or your chosen stopping rule
          importance = "permutation"
     )
  )

g_enet_f =
  prep_graph %>>%
  po("learner",
     lrn("classif.glmnet", predict_type = "prob",
         alpha = to_tune(0, 1),
         s     = to_tune(1e-4, 1, logscale = TRUE))
  )


auto = function(graph, id)
{
  at = AutoTuner$new(
    learner = GraphLearner$new(graph, id = id),
    resampling = rsmp("cv", folds = 5),
    measure = msr("classif.bbrier"),
    tuner = tnr("random_search"),
    terminator = trm("evals", n_evals = 1)
  )
  invisible(at)
}

at_lgbm_f = auto(g_lgbm_f, "lgbm")
at_xgb_f = auto(g_xgb_f, "xgb")
at_enet_f = auto(g_enet_f, "enet")
at_rf_f = auto(g_rf_f, "ranger")
at_featureless_f = auto(g_featureless_f, "featureless")


tasks_f = list(task_freq)
learners_f = list(at_lgbm_f, at_xgb_f, at_enet_f, at_featureless_f, at_rf_f)
design_f = benchmark_grid(task = tasks_f, learners = learners_f,
                        resampling = rsmp("cv", folds = 5))

kør_igen <- "nej"


if (!file.exists("~/Interpretable-Machine-Learning/aflevering/benchmark_frekvens.rds" ) | kør_igen == "ja") {
  bmr_f = benchmark(design_f, store_models = TRUE)
  saveRDS(bmr_f, "~/Interpretable-Machine-Learning/aflevering/benchmark_frekvens.rds")
} else {
  bmr_f = readRDS("~/Interpretable-Machine-Learning/aflevering/benchmark_frekvens.rds")
}


```


```{r combined_graph_plot_frek, fig.cap = "Input and model flow",  echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.width=12, fig.height=8, dpi=300}


prep_graph = po("encode") %>>% po("scale")

learners = list(
  po("learner", lrn("classif.featureless", predict_type = "prob"), id = "featless"),
  po("learner", lrn("classif.glmnet", predict_type = "prob"), id = "enet"),
  po("learner", lrn("classif.xgboost", predict_type = "prob"), id = "xgb"),
  po("learner", lrn("classif.lightgbm", predict_type = "prob"), id = "lgbm"),
  po("learner", lrn("classif.ranger", predict_type = "prob"), id = "ranger")
)

learners_union = gunion(learners)
combined_graph = prep_graph %>>% learners_union

#combined_graph = prep_graph %>>% learners_union
combined_graph$plot(html = FALSE)

```


Next we show the parameter space in table "**Parameter search space frequency**". 
```{r parameter_space_frek, echo = FALSE}


param_tbl <- tibble(
  Model                 = c("XGBoost",      "LightGBM",      "glmnet",     "Ranger"),
  `learning_rate / eta` = c("0.01 – 0.3",   "1e-3 – 0.2",    "",           ""),
  `leaves / depth`      = c("3 – 7 (depth)","16 – 32 (leaves)","",          ""),
  `iterations / trees`  = c("200 – 800",    "200 – 1000",    "",           ""),
  `subsample/ min_nodes`             = c("0.6 – 1",      "",              "",           "5"),
  `alpha / mtry`                = c("",             "",              "0 – 1",      "5"),
  s                     = c("",             "",              "1e-4 – 1",   "") )

gt(param_tbl) |>
  tab_header(title = md("**Parameter search space frequency**"))



```

The results of the nested cross-validation are shown in figure \ref{fig:boxplot_frek_kun}. 

```{r boxplot_frek_kun,  fig.cap = "MSE boxplot", echo = FALSE}

autoplot(bmr_f, measure = msr("classif.bbrier"), type = "boxplot")

```


```{r, echo = FALSE, include = FALSE}


invisible({
  
bm_agg     = bmr_f$aggregate(msr("classif.bbrier"))
best_row   = which.min(bm_agg$classif.bbrier)

rr         = bmr_f$resample_result(best_row)
fold_best  = which.min(rr$score(msr("classif.bbrier"))$classif.bbrier)


auto_best  = rr$learners[[fold_best]]
auto_best$tuning_result



best_params_f = auto_best$learner$param_set$values   
})

```

We show the best parameters in table "**Best parameters frequency model**". 


```{r best_parameters_frek, echo = FALSE}

gt::gt(enframe(best_params_f, name = "parameter", value = "value")) %>%
  tab_header(
    title = md("**Best parameters frequency model**")
  )

```








We have fitted the model with nested cross-validation since it provides a more stable estimate of the generalisation error.
Next we look at some model diagnostics.
We can look at the combined *bbrier* score for the aggregated model:
```{r, echo = FALSE}
print(rr$aggregate(msr("classif.bbrier")))
```
which seems quite small.

Which in general show that the accuracy and precision is very high, meaning that we are usually correct when predicting a claim.
We see that the True positive ratio  is lower than the $PPV = \frac{\text{TP}}{\text{TP}+ \text{FP}}$, indicating that we miss a fair share of claims.
Overall we note that our model seems stable through the cross-validation and that it performs somewhat well.


Here we see the *acc* error based on the choosen probability threshold, which as expected shows that for example a threshold at $0.5$ results in a pretty good model.
By setting the threshold higher we could risk introducing more false negatives.

Next we look at the severity model. For this model, we choose to log transform the response.
```{r, echo = FALSE, results=FALSE}

data_S <- data_trans
data_S <- data_S %>% dplyr::select(-claim_indicator, -ID)
data_S <- data_S %>% dplyr::filter(Cost_claim_this_year > 0)
data_S <- data_S %>% mutate(Cost_claim_this_year = log(Cost_claim_this_year))

task_S = as_task_regr(
  data_S,
  target = "Cost_claim_this_year",
  weights = "Exposure",
  id     = "severity"
)

task_S$set_col_roles("Exposure", "weight")


prep_graph = po("encode") %>>%
  po("scale")

g_featureless_s =
  prep_graph %>>%
  po("learner",
     lrn("regr.featureless")
  )



g_lgbm_s =
  prep_graph %>>%
  po("learner",
     lrn("regr.lightgbm",
        objective     = "regression",
         #metric        = "rmse",
         learning_rate          = to_tune(1e-3, 0.2, logscale = TRUE),
         num_leaves             = to_tune(16L, 32L),
         num_iterations         = to_tune(200L, 1000L))
  )


g_xgb_s =
  prep_graph %>>%
  po("learner",
     lrn("regr.xgboost",
        objective = "reg:squarederror",
         eta                    = to_tune(1e-3, 0.2, logscale = TRUE),
         max_depth              = to_tune(3L, 9L),
         nrounds                = to_tune(200L, 1000L))
  )

g_enet_s =
  prep_graph %>>%
  po("learner",
     lrn("regr.glmnet",
         alpha = to_tune(0, 1),
         s     = to_tune(1e-4, 1, logscale = TRUE))
  )

g_ranger_s <- prep_graph %>>%
  po("learner",
     lrn("regr.ranger",
         mtry = 5,
         min.node.size = 5,  # or your chosen stopping rule
          importance = "permutation"
     )
  )

auto = function(graph, id)
{
  at_s = AutoTuner$new(
    learner = GraphLearner$new(graph, id = id),
    resampling = rsmp("cv", folds = 5),
    measure = msr("regr.mse"),
    tuner = tnr("random_search"),
    terminator = trm("evals", n_evals = 1)
  )
  invisible(at_s)
}

at_lgbm_s = auto(g_lgbm_s, "lgbm")
at_xgb_s = auto(g_xgb_s, "xgb")
at_enet_s = auto(g_enet_s, "enet")
at_featureless_s = auto(g_featureless_s, "featureless")
at_ranger_s = auto(g_ranger_s, "ranger")

tasks_s = list(task_S)
learners_s = list(at_lgbm_s, at_xgb_s, at_enet_s,at_ranger_s, at_featureless_s)
#learners_s = list(at_ranger, at_featureless_s)
design_s = benchmark_grid(task = tasks_s, learners = learners_s,
                        resampling = rsmp("cv", folds = 5))

kør_igen <- "nej"


if (!file.exists("~/Interpretable-Machine-Learning/aflevering/benchmark_severity.rds" ) | kør_igen == "ja") {
  bmr_s = benchmark(design_s, store_models = TRUE)
  saveRDS(bmr_s, "~/Interpretable-Machine-Learning/aflevering/benchmark_severity.rds")
} else {
  bmr_s = readRDS("~/Interpretable-Machine-Learning/aflevering/benchmark_severity.rds")
}


```
We show the parameter space in table "**Parameter search space severity**". 


The results of the nested cross-validation are shown in figure \ref{fig:boxplot_severity_kun}. 


```{r boxplot_severity_kun, fig.cap = "MSE boxplot", echo = FALSE}

autoplot(bmr_s, measure = msr("regr.mse"), type = "boxplot")

```


```{r parameter_space_severity, echo = FALSE}


max_mtry <- as.integer(sqrt(ncol(data_S) - 2))

param_tbl <- tibble(
  Model                 = c("XGBoost",      "LightGBM",      "glmnet",     "Ranger"),
  `learning_rate / eta` = c("1e-3 – 0.2",   "1e-3 – 0.2",    "",           ""),
  `leaves / depth`      = c("3 – 9 (depth)","16 – 32 (leaves)","",          ""),
  `iterations / trees`  = c("200 – 1000",    "200 – 1000",    "",           ""),
  `subsample/ min_nodes`             = c("",      "",              "",           "5"),
  `alpha / mtry`                = c("",             "",              "0 – 1",      "5"),
  s                     = c("",             "",              "1e-4 – 1",   "") )
gt(param_tbl) |>
  tab_header(title = md("**Parameter search space severity**"))



```


```{r, echo = FALSE, include = FALSE}


invisible({
  
bm_agg     = bmr_s$aggregate(msr("regr.mse"))
best_row   = which.min(bm_agg$regr.mse)

rr         = bmr_s$resample_result(best_row)
fold_best  = which.min(rr$score(msr("regr.mse"))$regr.mse)


auto_best  = rr$learners[[fold_best]]
auto_best$tuning_result

best_params_s = auto_best$learner$param_set$values   
})

trained_learner = auto_best$learner$model

# trained_learner$regr.ranger$model
# 
# importance_values <- trained_learner$regr.ranger$model$variable.importance
# print(importance_values)

```
We show the best parameters in table "**Best parameters severity model**". 


```{r best_parameters_skade, echo = FALSE}

gt::gt(enframe(best_params_s, name = "parameter", value = "value")) %>%
  tab_header(
    title = md("**Best parameters severity model**")
  )

```


```{r funktioner til endelig resultat, echo = FALSE}

  train_tweedie <- function(data, folds, n_evals){
    
    task_twd = as_task_regr(
      data,
      target = "Cost_claim_this_year",
      weights = data$Exposure,
      id     = "twd"
    )
    
    task_twd$set_col_roles("Exposure", "weight")
    
    
    # Set claim_indicator as the stratum variable
    task_twd$col_roles$stratum = "claim_indicator"
    
    # Exclude claim_indicator from features (so it won't be used for training)
    task_twd$col_roles$feature = setdiff(task_twd$col_roles$feature, "claim_indicator")
    
    prep_graph = po("encode") %>>%
      po("scale")
    
    g_twd =
      prep_graph %>>%
      po("learner",
         lrn("regr.lightgbm",
             objective              = "tweedie",
             tweedie_variance_power = to_tune(1, 1.9),
             learning_rate          = to_tune(1e-3, 0.2, logscale = TRUE),
             num_leaves             = to_tune(16L, 32L),
             num_iterations         = to_tune(200L, 1000L))
      )
    
    
    glrn_twd = GraphLearner$new(g_twd)
    
    at_twd = AutoTuner$new(
      learner    = glrn_twd,
      resampling = rsmp("cv", folds = folds),  
      measure    = msr("regr.mse"),
      tuner      = tnr("random_search"),
      terminator = trm("evals", n_evals = n_evals)  
    )
    
    
    model_twd <- at_twd$train(task_twd)
    return(model_twd) 
  }
  
  # funk til frekvens -------------------
  train_freq <- function(data, folds, n_evals){
    
    data <- data %>% dplyr::select(-Cost_claim_this_year)
    
    task_freq = as_task_classif(
      data,
      target = "claim_indicator",
      positive = "1",
      weights = data$Exposure,
      id     = "frek_binary"
    )
  #task_freq$set_col_roles("Exposure", "weight")
  
  graph_freq  = po("encode") %>>%
    po("scale")  %>>%
    lrn("classif.xgboost",
        predict_type = "prob",
        eval_metric  = "logloss", #Bedre loss funktion?
        nrounds      = to_tune(200, 800),
        max_depth    = to_tune(3, 7),
        eta          = to_tune(0.01, 0.3),
        subsample    = to_tune(0.6, 1))
  
  at_freq = auto_tuner(
    learner    = as_learner(graph_freq),
    resampling = rsmp("cv", folds = folds), #ÆNDRET 5 -> 2
    measure    = msr("classif.bbrier"),
    tuner      = tnr("random_search"),
    term_evals = n_evals #Ændret 5 til 1
  )
  model_freq <- at_freq$train(task_freq)
    return(model_freq)
  }
  
  # funk til skader --------------- 
  train_severity <- function(data, folds, n_evals){
    data <- data %>% dplyr::select(-claim_indicator)
    data <- data %>% dplyr::filter(Cost_claim_this_year > 0)
    data <- data %>% mutate(Cost_claim_this_year = log(Cost_claim_this_year))
    
    task_S = as_task_regr(
      data,
      target = "Cost_claim_this_year",
      weights = data$Exposure,
      id     = "skade"
    )
    
    task_S$set_col_roles("Exposure", "weight")
    
    graph_S <- prep_graph %>>%
      po("learner",
         lrn("regr.ranger",
             mtry = 5,
             min.node.size = 5,  # or your chosen stopping rule
             importance = "permutation"
         )
      )

    
    glrn_S = GraphLearner$new(graph_S)
    
    at_severity = AutoTuner$new(
      learner    = glrn_S,
      resampling = rsmp("cv", folds = folds),  
      measure    = msr("regr.mse"),
      tuner      = tnr("random_search"),
      terminator = trm("evals", n_evals = n_evals)  
    )
    
    
    at_severity <- at_severity$train(task_S)
    return(at_severity) 
  }
```


Then we can combine the models, and calculate the mean square error for the combined model.


```{r , fig.cap="Predicted vs. actual", echo = FALSE}

final_eval <- FALSE

if (final_eval) {
  DATA <- data_trans
  N <- nrow(DATA)
  train_idx <- sample(seq_len(N), size = 0.8 * N)

  train <- DATA[train_idx, ]
  test <- DATA[-train_idx, ]

  tweedie_model <- train_tweedie(train, folds = 5, n_evals = 1)
  
  frekvens_model <- train_freq(train, folds = 5, n_evals = 1)

  
  skades_model <- train_severity(train, folds = 5, n_evals = 1)

  E_T <- tweedie_model$predict_newdata(test)
  E_N <- frekvens_model$predict_newdata(test)
  E_X <- skades_model$predict_newdata(test) #tilbage transformere predictions fra log skala


    res <- tibble(
    E_N = E_N$prob[, 1],
    E_X =  exp(E_X$response)
    )
    
  res_tweedie <- E_T$response

    
  res_combined <- res %>% dplyr::mutate(predicted = E_N * E_X )


  print("mse_tweedie")
  print(mean((res_tweedie-test$Cost_claim_this_year)^2))
  
  print("mse_combined_model")
  print(mean((res_combined$predicted-test$Cost_claim_this_year)^2))
  
  print("mse_baseline")
  print(mean((test$Cost_claim_this_year-mean(train$Cost_claim_this_year))^2))
}

print("Final model evaluation:

      mse_tweedie: 994964.6, 
      mse_combined_model: 1020423, 
      mse_baseline: 1030033")

```
In the above, we calculated MSE for tweedie, the splitted model, and for the baseline mean model trained on $80%$ of the data and tested for $20%$, 
We see here that our final model for the combined model is barely better than the baseline model predicting just the mean of the response.
It is clear that both our models (tweedie and frequency/severity model) perform extremely poorly, and one should probably have split up the claims into small and large claims so we could model the large and small claims independently.
Since both models are somewhat lacking we stick with the split model, since this is more common in the actuarial business.


## Discussion
Here we touch upon the various modeling problems.
##### Preprocessing
We took a somewhat minimalist approach: obvious date parsing, encoding etc. some slight imputation.
Since we impute, we impute off random values which adds noise.
We could have used better imputation, and our data aggregation could have been slightly better.


##### Model choices
In the frequency model, despite the good accuracy, we miss a lot of false negatives.
In the severity model, we severely miss the tail of the distribution.
Instead of splitting into two models, we could have made splitted into three models and used the variable in order to approximate:

$E(\frac{L}{E}) = E(F \mid Y>0) \cdot E\left(Y\mid Y >0 \right) \cdot P(Y>0)$

Where let $\frac{L}{E}$, $F$ is the Cost claims year. 


Also, we could have used the variable *R_claims_history* directly as a response variable in the frequency model, instead of creating our own binary claim indicator response variable. 

##### Evaluation protocol

We did nested cross-validation, but we could have increased the iterations and the number of folds for more stable results.
For severity we could have chosen a better metric to optimize for, which will hopefully result in a better model, especially in the tail.
We could have fit an EVT distribution on the heavy tailed data, but again this is quite strenious for this assignment.



