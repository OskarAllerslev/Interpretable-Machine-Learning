---
title: "Assignment 1"
author: "Jakob og Oskar"
date: "2025"
output:
  pdf_document:
    fig_caption: yes
---


```{r, echo = FALSE}

knitr::opts_chunk$set(
  echo   = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  dpi = 300,
  fig.width = 6, fig.height = 4
)
```


```{r, echo = FALSE, results=FALSE }
  data_trans <- function(data){

    today <- Sys.Date()

    date_cols <- c("Date_start_contract",
                   "Date_last_renewal",
                   "Date_next_renewal",
                   "Date_lapse")
    data[, (date_cols) := lapply(.SD, as.IDate, format = "%d/%m/%Y"),
         .SDcols = date_cols]
    data[, Period_end :=
           fifelse(
             !is.na(Date_lapse) &
               Date_lapse >= Date_last_renewal &
               Date_lapse <= Date_next_renewal,
             Date_lapse,
             pmin(Date_next_renewal, today, na.rm = TRUE)
           )]


    data[, Exposure_days  := as.numeric(Period_end - Date_last_renewal)]
    data[, Exposure_days  := pmax(Exposure_days, 0)]
    data[, Exposure_unit  := fcase(
      between(Exposure_days, 364, 367), 1,
      default = Exposure_days/365.25
    )]


    #data[, Cost_claims_sum := sum(Cost_claims_year), by = ID]

    # Add a column that flags the latest row per group

    # setorder(data, ID, -Date_last_renewal)
    # data[, is_second_latest := seq_len(.N) ==2, by = ID]
    # data[, is_second_latest := FALSE]
    # data[, is_second_latest := .I == .I[2], by = ID]
    # data$Cost_claims_year <- NA

    key_vars <- c("ID","Year_matriculation","Power",
                  "Cylinder_capacity","N_doors","Type_fuel","Weight", "Length",
                  "Area", "Value_vehicle", "Date_birth", "Date_driving_licence", "Date_start_contract")
    data$Date_birth <- as.integer(
      base::format(
        base::as.Date(data$Date_birth, format = "%d/%m/%Y"),
        "%Y"
      )
    )

    data$Date_driving_licence <- as.integer(
      base::format(
        base::as.Date(data$Date_driving_licence, format = "%d/%m/%Y"),
        "%Y"
      )
    )
    data$Date_start_contract <- as.integer(
      base::format(
        base::as.Date(data$Date_start_contract, format = "%d/%m/%Y"),
        "%Y"
      )
    )
    data <- data %>% dplyr::filter(Exposure_unit != 0)
    agg <- data[, .(
      Exposure               = sum(Exposure_unit),
      Max_policies_max       = max(Max_policies),
      # Policies_in_force_second_latest = Policies_in_force[is_second_latest],
      Max_lapse              = max(Lapse),
      Max_Date_birth         = max(Date_birth),
      Max_Date_driving_licence = max(Date_driving_licence),
      Max_Date_start_contract = max(Date_start_contract),
      #Distribution_channel = as.integer(max(Distribution_channel)),
      dist_channel_0 = as.integer(any(Distribution_channel == "0")),
      dist_channel_1 = as.integer(any(Distribution_channel == "1")),
      dist_channel_2 = as.integer(any(!(Distribution_channel %in% c("0", "1")))),
      # max_dist_channel_0     = max(Distribution_channel ),
      Max_products_max       = max(Max_products),
      Payment_max            = max(Payment),
      # Premium_second_latest = Premium[is_second_latest],
      Mean_premium            = (sum(Premium) - Premium[which.max(Date_last_renewal)]) / (sum(Exposure_unit)),
      Cost_claim_this_year = Cost_claims_year[which.max(Date_last_renewal)],
      Cost_claims_sum_history = sum(Cost_claims_year) -(Cost_claims_year[which.max(Date_last_renewal)]),
      R_claims_history   = max(N_claims_history - N_claims_year)/sum(Exposure_unit),
      # N_claims_year_sum      = sum(N_claims_year),
      N_claims_history   = max(N_claims_history - N_claims_year),
      # R_claims_history_sum   = sum(R_Claims_history),
      Type_risk_max          = max(Type_risk),
      Value_vehicle_mean     = mean(Value_vehicle),
      N_doors_mean           = mean(N_doors),
      Length_sum             = sum(Length, na.rm = TRUE)
    ), by = key_vars]


   # agg <- agg %>%
   #   dplyr::mutate(
   #    dist_channel_0          = dplyr::if_else(Distribution_channel == "0", 1, 0),
   #    dist_channel_1          = dplyr::if_else(Distribution_channel == "1", 1, 0),
   #    dist_channel_2          = dplyr::if_else(!(Distribution_channel %in% c("0", "1")), 1, 0)
   #   )


    agg <- agg %>%
      dplyr::mutate(claim_indicator = dplyr::if_else(Cost_claims_sum_history + Cost_claim_this_year > 0, 1, 0))

    # tmp <- agg %>%
    #   dplyr::filter(is.na(Type_fuel))
    #
    # NA_fuel_Y <- sum(tmp$Cost_claims_sum_history + tmp$Cost_claims_this_year)
    # total_Y <- sum(agg$Cost_claims_sum_history + agg$Cost_claims_this_year)
    #
    # na_Y_ratio <- NA_fuel_Y / total_Y

    # agg <- agg %>%
    #   dplyr::filter(!is.na(Type_fuel))

    # impute the data
    agg <- agg %>%
      dplyr::mutate(Type_fuel = dplyr::if_else(Type_fuel == "P", 1, 0))

    length_model <- lm(
      Length ~ Cylinder_capacity + Weight + Value_vehicle,
      data = na.omit(agg)
    )
    fuel_model <- lm(
      Type_fuel ~ Cylinder_capacity + Weight + Value_vehicle,
      data = na.omit(agg)
    )

    agg <- agg %>%
      dplyr::mutate(
        pred_length = stats::predict(length_model, newdata = .),
        pred_type_fuel = stats::predict(fuel_model, newdata = .)
      ) %>%
      dplyr::mutate(
        Length = dplyr::if_else(is.na(Length), pred_length, Length),
        Type_fuel = dplyr::if_else(is.na(Type_fuel), pred_type_fuel, Type_fuel)
      ) %>%
      dplyr::select(-pred_length, -pred_type_fuel)


    return(agg)
  }


```








```{r, echo = FALSE, results = FALSE, warnings = FALSE}
library(rsample)
library(DiagrammeR)
library(gt)
library(skimr)
library(patchwork)
library(mlr3viz)
library(GGally)
library(tweedie)
library(mlr3extralearners)
library(Hmisc)
library(mice)
library(mlr3)
library(mlr3learners )
library(mlr3tuning)
library(mlr3mbo)
library(glmnet)
library(OpenML)
library(mlr3pipelines)
library(future)
library(magrittr)
library(data.table)
library(tidyverse)
library(corrr)
library(corrplot)
library(mlr3verse)
```

# Assignment 1 in interpretable machine learning
We are tasked with building a predictive regression model, with the best possible prediction.
This submission will be split up into several parts:

- Initial data preprocessing and overview
- Introduction into the mathematics
- Modelling and justification
- Comparative discussion

## Initial data preprocessing and overview

The given data has the form:

```{r, echo = FALSE}

data <- fread("Motor vehicle insurance data.csv", sep = ";")
print(colnames(data))

```



We are asked to predict the **Cost_claims_year** given the rest of the covariate-vector.
Initially it is important to note that out data is a classical insurance dataset, where we are given rows corresponding to insurance periode for a given contract.
There are several issues with this, since some contracts might overlap into multiple contracts, which can be identified by the ID. It is however very difficult to locate these policies, and we overlook this issue.

There are numerous char. vectors in the data, which can be seen here:
```{r, echo = FALSE, out.width="50%"}
print(str(data))

```
One could, model some of the char. vectors like proposed in the lectures, by

\begin{align*}
 X_i &= E(Y \mid X_i ) \\
 &=\int_Y y\ \mu(x, dy)
\end{align*}

where $\mu$ is the appropriate probability kernel, and we let $y$ be our response.
However, we choose to take the our char. vectors and round them to yearly values, which then has a ordinal ordering and can thus be used as features.
Further we take and one-hot encode *Distribution_channel* by creating three new features which are either $1$ or $0$. Same for the type of fuel.


Next there are some missing values.
```{r ,  X axis on right shows rows where x is missing, and on the left the amount of rows where it is missing.", echo = FALSE,results=FALSE }

data_with_na <- data %>%
  dplyr::select(dplyr::where(~base::anyNA(.x)))



```



```{r, out.width="50%" , fig.cap="Missing data plot, right axis shows numer of missing columns in that row, and the left axis show how many rows have this missingness pattern",echo = FALSE, results="hide", message=FALSE}

invisible(mice::md.pattern(data_with_na, plot = TRUE))
```

It becomes apparent that there are missing values in the length and type_fuel variable.
We can see that the missingness is overlapping in $1764$ rows. However the length variable suffers way heavier from missingnes compared to type_fuel.
In order to impute values, we assume the missingnes it completely at random for both features.
We consider correlated features to do imputation. We see from the correlation plot (fig. \ref{fig:corplot} ) that type_fuel is mostly correlated with Cylinder_capacity, Value_vehicle and Weight. The same goes for the feature Length. Therefore, we fit a multivariate linear regression model with these 3 covariates to predict both type_fuel and Length (using cbind in the response formula). Finally, we predict the missing values using this trained model.


Our respone variable **Cost_claims_year** suffers from a few extreme values. We decide to remove these values to later on acheive a better model fit. In fig. \ref{fig:outlier_plot}, **N_claims_year** is plotted against **Cost_claims_year**, where at least 5-10 extreme values of **Cost_claims_year** are spotted.

```{r outlier_plot, out.width="50%",  fig.cap = "Claim costs over number of claims ", echo = FALSE}
#Plotting
ggplot(data, aes(x = N_claims_year, y = Cost_claims_year)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  labs(
       x = "N_claims_year",
       y = "Cost_claims_year") +
  theme_minimal()

#Removing top 10 outliers

data <- data %>%
  arrange(desc(Cost_claims_year)) %>%
  slice(-(1:10))

```



Finally we look at the correlation between the covariates.
```{r corplot, out.width="70%",  fig.cap = "Correlation plot between the continious covariates", echo = FALSE}
num_data <- data %>%
  mutate(
    Type_fuel_num = case_when(
      Type_fuel == "P" ~ 1,
      Type_fuel == "D" ~ 0,
      TRUE             ~ NA_real_
    ),
    Length_num = as.numeric(Length)
  ) %>%

  select(
    -Date_start_contract, -Date_last_renewal, -Date_next_renewal,
    -Date_birth, -Date_lapse, -Date_driving_licence,
    -Distribution_channel,
    -Type_fuel,
    -Length
  ) %>%

  select(where(is.numeric))

M <- cor(num_data, use = "pairwise.complete.obs")
corrplot(M, order = "AOE")
```

We notice some clusters, most meaningfull between *Cylinder_capacity*, *Value_vehicle* and *Weight* which is expected. We deem these to have significant predictive ability, and thus we choose to not remove these.
The top left cluster, will be ignored for now, since we will later introduce a data-transformation which affects this cluster in a high degree.



##### Data Aggregation

For our data aggregation we want to have the ability to assume independence, which we have if we aggregate the rows into being one policy.
There are several problems to tacle to achive data in the desired format.
We have to both choose an appropriate aggreations function, and we have to consider events where the insured object change.
We start off by identifying some unique-identifiers.
If theme columns change, we deem the contract to insure a new car, or at least insure something that is independent of what was previously insured.
Amongst these unique identifiers are Length, Weight, Power etc. so things which change, probably mean that the thing that is insured has changes.
\\
Next, we look at how we have aggregated data. Firstly we sum the exposure for each contract, since we will use this to weight in the models later on.
Next we use the max function on the number of policies, lapse, date of birth, date of driving licence, start of contract, products, payments, ratio of claims history, type of risk,.
For The mean premium we use the mean of the premium without the forward premium, that is the premium for the time we want to predict for.
For the value of the veichle, and number of doors we use mean.
For the length we use sum since this is just a scaling.
For the variable *cost_claims_this_year* we use the cost of the claims in $\mathcal{F}_t$ where we are to predict data on $\mathcal{F}_{t+1}$.





So we have $Z_i(\omega_i) := (X_i(\omega_i), Y_i(\omega_i)): (\Omega_i, \mathcal{F}_i) \rightarrow ( \mathcal{X} \times \mathcal{Y}, \mathcal{B}(\mathcal{X} \times \mathcal{Y}))$, where by aggregating data row-wise we obtain independence on every set in the Borel-set induced.
However this is an empirical assumption which can be violated by catastrophe events.


## Introduction into the mathematical framework

Note the classical derivation of how to decompose a total claim sum. 
\begin{align*}
E(S(t) \mid Z=z) &= E\left(\sum_{i=1}^{N(t)} X_i \mid Z = z \right)\\
&= E\left( \left( \sum_{i=1}^{N(t)} X_i \mid Z = z \cap \mathcal{F}(t) \right)\right) \\
&=E\left( N E\left( X_i \mid Z = z \right)  \mid Z=z\right) \\
&= E(N \mid Z = z) E(X_1 \mid Z = z)
\end{align*}
We would however like to modify this slightly, later on. 
Further the general definition of the Tweedie law is  
\begin{align*}
P_{\theta, \sigma^2}(Y \in A) = \int_A \exp\left\{ \frac{\theta z - \kappa_p(\theta)}{\sigma^2} \right\}\nu_y(dz)
\end{align*}
Where $A \subset \mathbb{R}$ and measurable. $\theta$ is the canonical parameter, and $\kappa_p(\theta)$ is the cumulant function dependent on $p$ the so called tweedie power parameter. $\sigma^2$ is the dispersion parameters, and $\nu_\lambda$ is some sigma-finite base measure depending on the Lebesgue measure. 

since it can accommodate a zero-inflated distribution very well, we feel the need to introduce this model here.
Lastly we introduce the so-called *bbrier* measure, for classification models.

\begin{align*}
\frac{1}{n} \sum_{i=1}^n w_i(1\{X_i = 1 \} - P(X_i = 1))^2
\end{align*}

Which is the mean weighted square euclidean distance between the probability and the true value.


## Modelling and justification

In this section, we train and evauluate different models. 
For all the proposed models, we have caried out nested crossvalidition. 
We do stratified resampling in both the inner and outer cv loops, to ensure both zero and non-zero claims over the training and test set.
I final note, that goes for all our trained models, is that we introduce offset on the exposure. Meaning: Give more importance to observations with more exposure since they represent more information. 
```{r}
#Common trans dataset to skip rerunning trans

data_trans <- data_trans(data)

```


```{r, echo = FALSE, results = FALSE}

prep_graph = po("encode") %>>%
  po("scale")

g_featureless =
  prep_graph %>>%
  po("learner",
     lrn("regr.featureless")
  )

g_lgbm =
  prep_graph %>>%
  po("learner",
     lrn("regr.lightgbm",
         objective              = "tweedie",
         tweedie_variance_power = to_tune(1, 1.9),
         learning_rate          = to_tune(1e-3, 0.2, logscale = TRUE),
         num_leaves             = to_tune(16L, 32L),
         num_iterations         = to_tune(200L, 1000L))
  )

g_xgb =
  prep_graph %>>%
  po("learner",
     lrn("regr.xgboost",
         objective              = "reg:tweedie",
         tweedie_variance_power = to_tune(1, 1.9),
         eta                    = to_tune(1e-3, 0.2, logscale = TRUE),
         max_depth              = to_tune(3L, 9L),
         nrounds                = to_tune(200L, 1000L))              
  )

g_enet =
  prep_graph %>>%
  po("learner",
     lrn("regr.glmnet",
         alpha = to_tune(0, 1),
         s     = to_tune(1e-4, 1, logscale = TRUE))   
  )


auto = function(graph, id)
{
  at = AutoTuner$new(
    learner = GraphLearner$new(graph, id = id),
    resampling = rsmp("cv", folds = 5), 
    measure = msr("regr.mse"), 
    tuner = tnr("random_search"), 
    terminator = trm("evals", n_evals = 2)
  )
  invisible(at)
}

at_lgbm = auto(g_lgbm, "lgbm")
at_xgb = auto(g_xgb, "xgb")
at_enet = auto(g_enet, "enet")
at_featureless = auto(g_featureless, "featureless")


data_twd <- data_trans
data_twd <- data_twd %>%
  dplyr::select(-claim_indicator, - ID)

task_twd = as_task_regr(data_twd, 
                        target = "Cost_claim_this_year", 
                        weights = data_twd$Exposure, 
                        id = "tweedie")

task_twd$col_roles$stratum = "Cost_claim_this_year"


task = list(task_twd)
learners = list(at_lgbm, at_xgb, at_enet, at_featureless)
design = benchmark_grid(task = task, learners = learners, 
                        resampling = rsmp("cv", folds = 5))

kør_igen <- "nej"

if (!file.exists("~/Interpretable-Machine-Learning/aflevering/benchmark_tweedie.rds" ) | kør_igen == "ja") {
  bmr = benchmark(design, store_models = TRUE)
  saveRDS(bmr, "~/Interpretable-Machine-Learning/aflevering/benchmark_tweedie.rds")
} else {
  bmr = readRDS("~/Interpretable-Machine-Learning/aflevering/benchmark_tweedie.rds")
}


```



##### Tweedie
Initily we want to fit a Tweedie model on the entire data, since the Tweedie model is known for handeling zero-inflated distributions well.

We apply both the Xgboost, Ligth-gbm and glmnet. 
For each of these we supply search-space information in the **Parameter search space** Table. 


```{r parameter_space_tweedie, echo = FALSE}

gt::gt(tibble::tibble(
  Model = c("Xgboost", "lgbm", "glmnet"), 
  tweedie_power = c("1-1.9", "1-1.9", ""), 
  learning_rate_eta = c("1e-3 - 0.2", "1e-3 - 0.2", ""), 
  leaves = c("16 - 32", "3 - 9", ""), 
  iterations= c("200 - 1000", "200 - 1000", ""), 
  s = c("", "", "1e-4 - 1"), 
  alpha = c("", "", "0 - 1") 
  )
) %>%
  tab_header(
    title = md("**Parameter search space**")
  )


```

We define the workflow as shown in \ref{fig:combined_graph_plot}. 

```{r combined_graph_plot, fig.cap = "Input and model flow",  echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.width=12, fig.height=8, dpi=300}

# Individual learners
featureless_learner = po("learner", lrn("regr.featureless", id = "featureless"))
lgbm_learner = po("learner", lrn("regr.lightgbm", id = "lgbm"))
xgb_learner  = po("learner", lrn("regr.xgboost", id = "xgb"))
enet_learner = po("learner", lrn("regr.glmnet", id = "enet"))

combined_learners = gunion(list(featureless_learner, lgbm_learner, xgb_learner, enet_learner))

combined_graph = prep_graph %>>% combined_learners

combined_graph$plot()

```

The results of the nested cross-validation are shown in \ref{fig:boxplot_kun}. 

```{r boxplot_tweedie_kun,  fig.cap = "MSE boxplot", echo = FALSE}

autoplot(bmr, measure = msr("regr.mse"), type = "boxplot")
```


And finally the chosen hyperparameters are presented in Table **Best parameters tweedie**. 
```{r, echo = FALSE, include = FALSE}


invisible({
  
bm_agg     = bmr$aggregate(msr("regr.mse"))
best_row   = which.min(bm_agg$regr.mse)

rr         = bmr$resample_result(best_row)
fold_best  = which.min(rr$score(msr("regr.mse"))$regr.mse)

auto_best  = rr$learners[[fold_best]]

best_params = auto_best$learner$param_set$values   
})

```

```{r best_parameters_tweedie, echo = FALSE}

gt::gt(enframe(best_params, name = "parameter", value = "value")) %>%
  tab_header(
    title = md("**Best parameters tweedie**")
  )

```


vi skal skrive noget mere til ovenstående, når resultater er på plads!



##### Split model
Next we can look at the split model proposed in the mathematical frame, where we aim to create two models, and then predict in the following manner:
\begin{align*}
P(N > 0 \mid Z) \cdot  E(X \mid Z)
\end{align*}

Which is unlike the traditional and proposed $E(N)E(X)$, but since we are asked to predict the next year, we feel like we can provide a suitable approximation with the above prediction.
This entails fitting both a frequency model and a severity model.
Hence we begin with the frequency regression model.

```{r, echo = FALSE, results=FALSE}

data_F <- data_trans
data_F <- data_F %>%
  dplyr::select(-Cost_claim_this_year, - ID)


folds_inner <- 2
folds_outer <- 2
n_evals     <- 1


skim(data_F)

task_freq <- as_task_classif(
  data_F,
  target = "claim_indicator",
  positive = "1",
  weights = data_F$Exposure,
  id = "frek_classif"
)

# stratificer tjek at det faktisk er hvad der sker her
task_freq$col_roles$stratum = "claim_indicator"


prep_graph = po("encode") %>>%
  po("scale")

g_featureless_f =
  prep_graph %>>%
  po("learner", lrn("classif.featureless", predict_type = "prob"))

g_rf_f = po("encode") %>>%
  po("learner", lrn("classif.ranger", predict_type = "prob"))


g_lgbm_f =
  prep_graph %>>%
  po("learner",
     lrn("classif.lightgbm",
         objective              = "binary",
         learning_rate          = to_tune(1e-3, 0.2, logscale = TRUE),
         num_leaves             = to_tune(16L, 32L),
         num_iterations         = to_tune(200L, 1000L))
  )

g_xgb_f =
  prep_graph %>>%
  po("learner",
     lrn("classif.xgboost",
      predict_type = "prob",
      eval_metric = "logloss",
      nrounds      = to_tune(200, 800),
      max_depth    = to_tune(3, 7),
      eta          = to_tune(0.01, 0.3),
      subsample    = to_tune(0.6, 1)
  ))

g_enet_f =
  prep_graph %>>%
  po("learner",
     lrn("classif.glmnet", predict_type = "prob",
         alpha = to_tune(0, 1),
         s     = to_tune(1e-4, 1, logscale = TRUE))
  )


auto = function(graph, id)
{
  at = AutoTuner$new(
    learner = GraphLearner$new(graph, id = id),
    resampling = rsmp("cv", folds = 2),
    measure = msr("classif.bbrier"),
    tuner = tnr("random_search"),
    terminator = trm("evals", n_evals = 1)
  )
  invisible(at)
}

at_lgbm_f = auto(g_lgbm_f, "lgbm")
at_xgb_f = auto(g_xgb_f, "xgb")
at_enet_f = auto(g_enet_f, "enet")
at_featureless_f = auto(g_featureless_f, "featureless")
at_rf_f = auto(g_rf_f, "ranger")


tasks_f = list(task_freq)
learners_f = list(at_lgbm_f, at_xgb_f, at_enet_f, at_featureless_f, at_rf_f)
design_f = benchmark_grid(task = tasks_f, learners = learners_f,
                        resampling = rsmp("cv", folds = 2))

kør_igen <- "nej"


if (!file.exists("~/Interpretable-Machine-Learning/aflevering/benchmark_frekvens.rds" ) | kør_igen == "ja") {
  bmr_f = benchmark(design_f, store_models = TRUE)
  saveRDS(bmr_f, "~/Interpretable-Machine-Learning/aflevering/benchmark_frekvens.rds")
} else {
  bmr_f = readRDS("~/Interpretable-Machine-Learning/aflevering/benchmark_frekvens.rds")
}


```

```{r boxplot_frek_kun,  fig.cap = "MSE boxplot", echo = FALSE}

autoplot(bmr_f, measure = msr("classif.bbrier"), type = "boxplot")

```

We have fitted the model with nested cross-validation since it provides a more stable estimate of the generalisation error.
We use $K = 2$ folds in both the inner and outer loops.
Next we look at some model diagnostics.
We can look at the combined *bbrier* score for the aggregated model:
```{r, echo = FALSE}
print(rr$aggregate(msr("classif.bbrier")))
```
which seems quite small.
Further we have some confusion matrices for each of the folds.
```{r, echo = FALSE}

ncs_plot_dat <- rr$predictions()

for (i in 1:length(ncs_plot_dat)){
  cat("Fold: ", i)
  print(mlr3measures::confusion_matrix(
    truth = ncs_plot_dat[[i]]$truth,
    response = ncs_plot_dat[[i]]$response,
    positive = "1"

  ))
}


```


Which in general show that the accuracy and precision is very high, meaning that we are usually correct when predicting a claim.
We see that the True positive ratio  is lower than the $PPV = \frac{\text{TP}}{\text{TP}+ \text{FP}}$, indicating that we miss a fair share of claims.
Overall we note that our model seems stable through the cross-validation and that it performs somewhat well.


```{r , fig.cap = "the bbrier error for each choice of probability threshold in classification", echo = FALSE}
#Skal vi helt droppe denne?
#autoplot(ncs_plot_dat[[1]], type = "threshold", measure = msr("classif.acc"))

```

Here we see the *acc* error based on the choosen probability threshold, which as expected shows that for example a threshold at $0.5$ results in a pretty good model.
By setting the threshold higher we could risk introducing more false negatives.

Next we look at the severity model.
```{r, echo = FALSE, results=FALSE}

data_S <- data_trans
data_S <- data_S %>% dplyr::select(-claim_indicator, -ID)
data_S <- data_S %>% dplyr::filter(Cost_claim_this_year > 0)
data_S <- data_S %>% mutate(Cost_claim_this_year = log(Cost_claim_this_year))

hist(data_S$Cost_claim_this_year)

folds_inner <- 2
folds_outer <- 2
n_evals     <- 1


task_S = as_task_regr(
  data_S,
  target = "Cost_claim_this_year",
  weights = data_S$Exposure,
  id     = "skade"
)

prep_graph = po("encode") %>>%
  po("scale")

g_featureless_s =
  prep_graph %>>%
  po("learner",
     lrn("regr.featureless")
  )



g_lgbm_s =
  prep_graph %>>%
  po("learner",
     lrn("regr.lightgbm",
         objective              = "gamma",
         learning_rate          = to_tune(1e-3, 0.2, logscale = TRUE),
         num_leaves             = to_tune(16L, 32L),
         num_iterations         = to_tune(200L, 1000L))
  )


g_xgb =
  prep_graph %>>%
  po("learner",
     lrn("regr.xgboost",
         objective              = "reg:gamma",
         eta                    = to_tune(1e-3, 0.2, logscale = TRUE),
         max_depth              = to_tune(3L, 9L),
         nrounds                = to_tune(200L, 1000L))
  )

g_enet =
  prep_graph %>>%
  po("learner",
     lrn("regr.glmnet",
         alpha = to_tune(0, 1),
         s     = to_tune(1e-4, 1, logscale = TRUE))
  )


auto = function(graph, id)
{
  at = AutoTuner$new(
    learner = GraphLearner$new(graph, id = id),
    resampling = rsmp("cv", folds = 2),
    measure = msr("regr.mse"),
    tuner = tnr("random_search"),
    terminator = trm("evals", n_evals = 1)
  )
  invisible(at)
}

at_lgbm = auto(g_lgbm, "lgbm")
at_xgb = auto(g_xgb, "xgb")
at_enet = auto(g_enet, "enet")
at_featureless = auto(g_featureless, "featureless")


tasks_s = list(task_S)
learners_s = list(at_lgbm, at_xgb, at_enet, at_featureless)
design_s = benchmark_grid(task = tasks_s, learners = learners_s,
                        resampling = rsmp("cv", folds = 2))

kør_igen <- "nej"


if (!file.exists("~/Interpretable-Machine-Learning/aflevering/benchmark_severity.rds" ) | kør_igen == "ja") {
  bmr_s = benchmark(design, store_models = TRUE)
  saveRDS(bmr_s, "~/Interpretable-Machine-Learning/aflevering/benchmark_severity.rds")
} else {
  bmr_s = readRDS("~/Interpretable-Machine-Learning/aflevering/benchmark_severity.rds")
}


```


```{r , fig.cap = "Shows the predicted values along the x-axis vs the real values on the y-axis", echo = FALSE}

autoplot(bmr_s, measure = msr("regr.mse"), type = "boxplot")


```


Then we can combine the models, and calculate the mean square error for the combined model.
We have shown that our models are somewhat acceptable, and we not fit them on the entire dataset and find the out of sample *MSE* for $20$ percent of data.


```{r , fig.cap="Predicted vs. actual", echo = FALSE,message = FALSE, warning = FALSE, results = 'hold' }

final_eval <- FALSE

if (final_eval) {
  DATA <- data_trans
  N <- nrow(DATA)
  train_idx <- sample(seq_len(N), size = 0.8 * N)

  train <- DATA[train_idx, ]
  test <- DATA[-train_idx, ]

  #Funktion train_freq findes i "train_predict_submit"
  frekvens_model <- train_freq(train, folds = 2, n_evals = 1)

  #Funktion train_severity findes i "train_predict_submit"
  skades_model <- train_severity(train, folds = 2, n_evals = 1)


  E_N <- frekvens_model$predict_newdata(test)
  E_X <- skades_model$predict_newdata(test)

  res <- tibble(
    E_N = E_N$prob[, 1],
    E_X =  E_X$response
    )

  res <- res %>% dplyr::mutate(predicted = E_N * E_X )


  plot(res$predicted, test$Cost_claim_this_year)
  print(mse_oos <- mean((res$predicted-test$Cost_claim_this_year)^2))
}

```
We see here that our final model for the combined model.
It is clear that both our models perform extremely poorly, and one should probably have split up the claims into small and large claims so we could model the large and small claims independently.
Since both models are somewhat lacking we stick with the split model, since this is more common in the actuarial business.




## Discussion
Here we touch upon the various modeling problems.
##### Pre-processing
We took a somewhat minimalist approach: obvious date parsing, encoding etc. some slight imputation.
Since we impute, we impute off random values which adds noise.
We could have used better imputation, and our data aggregation could have been slightly better.



##### Model choices
In the frequency model, despite the good accuracy, we miss a lot of false negatives.
In the severity model, we severly miss the tail of the distribution.

##### Evaluation protocol

We did nested cross-validation, but we could have increased the iterations and the number of folds for more stable results.
For severity we could have chosen a better metric to optimize for, which will hopefully result in a better model, especially in the tail.
We could have fit an EVT distribution on the heavy tailed data, but again this is quite strenious for this assignment.







